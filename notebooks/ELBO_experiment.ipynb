{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effect of Amortisation on Aquired ELBO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "# import argparse\n",
    "# import json\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import wbml.plot\n",
    "import wbml.experiment\n",
    "\n",
    "import bnn_amort_inf\n",
    "\n",
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params to be changed in experiment\n",
    "num_datasets = 1\n",
    "seed = 0\n",
    "\n",
    "# data params\n",
    "kernel = \"se\"\n",
    "x_min = -2.0\n",
    "x_max = 2.0\n",
    "n_min_train = 10\n",
    "n_max_train = 50\n",
    "n_min_test = 10\n",
    "n_max_test = 50\n",
    "num_test_datasets = 5\n",
    "\n",
    "# model params\n",
    "hidden_dims = [32, 32]\n",
    "in_hidden_dims = [50, 50]\n",
    "min_num_inducing = 20\n",
    "\n",
    "# training params\n",
    "loss_fn = \"loss\"\n",
    "amort_lr = 1e-3\n",
    "reg_lr = 5e-3\n",
    "max_iters = 15_000\n",
    "batch_size = 5\n",
    "min_es_iters = 1000\n",
    "ref_es_iters = 500\n",
    "smooth_es_iters = 500\n",
    "\n",
    "include_gibnn = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Test Datasets (fixed seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "test_datasets = bnn_amort_inf.utils.dataset_utils.gen_datasets(\n",
    "    num_datasets=num_test_datasets,\n",
    "    kernel=kernel,\n",
    "    x_min=x_min,\n",
    "    x_max=x_max,\n",
    "    n_min=n_min_test,\n",
    "    n_max=n_max_test,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Training Datasets (variable seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "\n",
    "train_metadataset = bnn_amort_inf.utils.dataset_utils.gen_datasets(\n",
    "    num_datasets=num_datasets,\n",
    "    kernel=kernel,\n",
    "    x_min=x_min,\n",
    "    x_max=x_max,\n",
    "    n_min=n_min_train,\n",
    "    n_max=n_max_train,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct Metamodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agibnn = bnn_amort_inf.models.bnn.gibnn.AmortisedGIBNN(\n",
    "    x_dim=1,\n",
    "    y_dim=1,\n",
    "    hidden_dims=hidden_dims,\n",
    "    in_hidden_dims=in_hidden_dims,\n",
    "    likelihood=bnn_amort_inf.models.likelihoods.normal.NormalLikelihood(\n",
    "        noise=0.05, train_noise=False\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Metamodel on Metadataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if num_datasets > 0:\n",
    "    agibnn_tracker = bnn_amort_inf.utils.training_utils.train_metamodel(\n",
    "        agibnn,\n",
    "        dataset=train_metadataset,\n",
    "        loss_fn=loss_fn,\n",
    "        lr=amort_lr,\n",
    "        max_iters=max_iters,\n",
    "        batch_size=batch_size,\n",
    "        min_es_iters=min_es_iters,\n",
    "        ref_es_iters=ref_es_iters,\n",
    "        smooth_es_iters=smooth_es_iters,\n",
    "    )\n",
    "\n",
    "    for k, v in agibnn_tracker.items():\n",
    "        # Plot results and save.\n",
    "        plt.figure(figsize=(4, 1.5), dpi=200)\n",
    "\n",
    "        plt.plot(v)\n",
    "        plt.ylabel(k)\n",
    "        wbml.plot.tweak()\n",
    "        plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate performance on test datasets.\n",
    "x_test = torch.linspace(x_min, x_max, 1000).unsqueeze(-1)\n",
    "for i, dataset in enumerate(test_datasets):\n",
    "    x, y = dataset\n",
    "\n",
    "    # Train metamodel on just test datasets if num_datasets == 0\n",
    "    if num_datasets == 0:\n",
    "        agibnn = bnn_amort_inf.models.bnn.gibnn.AmortisedGIBNN(\n",
    "            x_dim=1,\n",
    "            y_dim=1,\n",
    "            hidden_dims=hidden_dims,\n",
    "            in_hidden_dims=in_hidden_dims,\n",
    "            likelihood=bnn_amort_inf.models.likelihoods.normal.NormalLikelihood(\n",
    "                noise=0.05, train_noise=False\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        agibnn_tracker = bnn_amort_inf.utils.training_utils.train_model(\n",
    "            agibnn,\n",
    "            dataset=torch.utils.data.TensorDataset(x, y),\n",
    "            batch_size=x.shape[0],\n",
    "            lr=amort_lr,\n",
    "            max_iters=max_iters,\n",
    "            min_es_iters=min_es_iters,\n",
    "            ref_es_iters=ref_es_iters,\n",
    "            smooth_es_iters=smooth_es_iters,\n",
    "        )\n",
    "\n",
    "        for k, v in agibnn_tracker.items():\n",
    "            # Plot results and save.\n",
    "            plt.figure(figsize=(4, 1.5), dpi=200)\n",
    "\n",
    "            plt.plot(v)\n",
    "            plt.ylabel(k)\n",
    "            wbml.plot.tweak()\n",
    "            plt.show()\n",
    "\n",
    "    # Evaluate amortised BNN on test datasets\n",
    "    with torch.no_grad():\n",
    "        agibnn_pred_samples = agibnn(x, y, x_test=x_test, num_samples=100)[-1]\n",
    "\n",
    "        agibnn_loss, _ = agibnn.loss(x, y, num_samples=100)\n",
    "        agibnn_elbo = (-agibnn_loss) * x.shape[0]\n",
    "        agibnn_metrics = {\"elbo\": agibnn_elbo.item()}\n",
    "\n",
    "    agibnn_pred_loc = agibnn_pred_samples.mean(0)\n",
    "    agibnn_pred_std = agibnn_pred_samples.std(0)\n",
    "\n",
    "    # Train regular BNN on test datasets\n",
    "    if include_gibnn:\n",
    "        inducing_points = x.clone()\n",
    "\n",
    "        if min_num_inducing > x.shape[0]:\n",
    "            inducing_points = torch.cat(\n",
    "                [\n",
    "                    inducing_points,\n",
    "                    torch.linspace(\n",
    "                        x_min, x_max, min_num_inducing - x.shape[0]\n",
    "                    ).unsqueeze(-1),\n",
    "                ],\n",
    "                dim=0,\n",
    "            )\n",
    "\n",
    "        gibnn = bnn_amort_inf.models.bnn.gibnn.GIBNN(\n",
    "            x_dim=1,\n",
    "            y_dim=1,\n",
    "            hidden_dims=hidden_dims,\n",
    "            num_inducing=len(inducing_points),\n",
    "            inducing_points=inducing_points,\n",
    "            likelihood=bnn_amort_inf.models.likelihoods.normal.NormalLikelihood(\n",
    "                noise=0.05, train_noise=False\n",
    "            ),\n",
    "            final_layer_prec=1e0,\n",
    "        )\n",
    "\n",
    "        gibnn_tracker = bnn_amort_inf.utils.training_utils.train_model(\n",
    "            gibnn,\n",
    "            dataset=torch.utils.data.TensorDataset(x, y),\n",
    "            batch_size=x.shape[0],\n",
    "            lr=reg_lr,\n",
    "            max_iters=max_iters,\n",
    "            min_es_iters=min_es_iters,\n",
    "            ref_es_iters=ref_es_iters,\n",
    "            smooth_es_iters=smooth_es_iters,\n",
    "        )\n",
    "\n",
    "        for k, v in gibnn_tracker.items():\n",
    "            # Plot results and save.\n",
    "            plt.figure(figsize=(4, 1.5), dpi=200)\n",
    "\n",
    "            plt.plot(v)\n",
    "            plt.ylabel(k)\n",
    "            wbml.plot.tweak()\n",
    "            plt.show()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            gibnn_loss, _ = gibnn.loss(x, y, num_samples=100)\n",
    "            gibnn_elbo = (-gibnn_loss) * x.shape[0]\n",
    "            gibnn_metrics = {\"elbo\": gibnn_elbo.item()}\n",
    "\n",
    "            gibnn_pred_samples = gibnn(x_test, num_samples=100)[0]\n",
    "\n",
    "        gibnn_pred_loc = gibnn_pred_samples.mean(0)\n",
    "        gibnn_pred_std = gibnn_pred_samples.std(0)\n",
    "    else:\n",
    "        gibnn_metrics = None\n",
    "        gibnn_pred_loc = None\n",
    "        gibnn_pred_std = None\n",
    "\n",
    "    print(\n",
    "        \"kernel: \",\n",
    "        kernel,\n",
    "        \", test dataset: {}\".format(i),\n",
    "        \"gibnn metrics: \",\n",
    "        gibnn_metrics,\n",
    "        \"agibnn_metrics: \",\n",
    "        agibnn_metrics,\n",
    "    )\n",
    "\n",
    "    # Plot results and save.\n",
    "    plt.figure(figsize=(4, 1.5), dpi=200)\n",
    "    plt.scatter(x, y, style=\"train\", zorder=2, s=10)\n",
    "\n",
    "    if agibnn_pred_loc is not None:\n",
    "        plt.plot(x_test, agibnn_pred_loc, style=\"pred\", ls=\"-\", lw=2, zorder=1)\n",
    "\n",
    "    if agibnn_pred_std is not None:\n",
    "        pred_upper = agibnn_pred_loc + 1.96 * agibnn_pred_std\n",
    "        pred_lower = agibnn_pred_loc - 1.96 * agibnn_pred_std\n",
    "        plt.fill_between(\n",
    "            x_test.squeeze(),\n",
    "            pred_upper.squeeze(),\n",
    "            pred_lower.squeeze(),\n",
    "            style=\"pred\",\n",
    "            alpha=0.2,\n",
    "            zorder=1,\n",
    "        )\n",
    "\n",
    "    if gibnn_pred_loc is not None:\n",
    "        plt.plot(x_test, gibnn_pred_loc, style=\"pred\", ls=\"-\", lw=2, zorder=1)\n",
    "\n",
    "    if gibnn_pred_std is not None:\n",
    "        pred_upper = gibnn_pred_loc + 1.96 * gibnn_pred_std\n",
    "        pred_lower = gibnn_pred_loc - 1.96 * gibnn_pred_std\n",
    "        plt.fill_between(\n",
    "            x_test.squeeze(),\n",
    "            pred_upper.squeeze(),\n",
    "            pred_lower.squeeze(),\n",
    "            style=\"pred\",\n",
    "            alpha=0.2,\n",
    "            zorder=1,\n",
    "        )\n",
    "\n",
    "    # if pred_samples is not None:\n",
    "    #     for sample in pred_samples:\n",
    "    #         plt.plot(x_test, sample, style=\"pred\", ls=\"-\", alpha=0.1, zorder=0)\n",
    "\n",
    "    plt.ylim([-2.5, 2.5])\n",
    "    wbml.plot.tweak()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bnn-amort-inf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
