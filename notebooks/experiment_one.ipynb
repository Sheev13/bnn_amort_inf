{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1\n",
    "### In this experiment we compare the predictive capabilities of three meta models across four types of dataset. \n",
    "We compare the new amortised global inducing point BNN with a vanilla neural process and an amortised MFVI BNN as two baseline models. The test datasets are:\n",
    "- Noisy sample from squared exponential covarince GP prior\n",
    "- Noisy sample from Laplacian covariance GP prior\n",
    "- Noisy sample from periodic covariance GP prior\n",
    "- Noisy cubic dataset with central gap\n",
    "\n",
    "For each dataset, a non meta model representing the 'ground truth' is used to compare predictions against. For the GP-generated datasets, the posterior from a hyperparameter-optimised GP with the corresponding covariance function is used, and for the cubic dataset, the vanilla global inducing point BNN is used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import gpytorch\n",
    "import tqdm.auto as tqdm\n",
    "import sys\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from bnn_amort_inf.models.bnn import gibnn, mfvi_bnn\n",
    "from bnn_amort_inf.models import gp, np\n",
    "from bnn_amort_inf import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate metadatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_datasets = 1000\n",
    "meta_datasets = {}\n",
    "kernels = [\"se\", \"per\", \"lap\"]\n",
    "\n",
    "x_min = -5.0\n",
    "x_max = 5.0\n",
    "\n",
    "for kernel in kernels:\n",
    "    meta_datasets[kernel] = utils.dataset_utils.MetaDataset(\n",
    "        [\n",
    "            utils.gp_datasets.gp_dataset_generator(\n",
    "                kernel=kernel, x_min=x_min, x_max=x_max\n",
    "            )\n",
    "            for _ in range(num_datasets)\n",
    "        ]\n",
    "    )\n",
    "meta_datasets[\"saw\"] = utils.dataset_utils.MetaDataset(\n",
    "    [\n",
    "        utils.dataset_utils.sawtooth_dataset(lower=x_min, upper=x_max)\n",
    "        for _ in range(num_datasets)\n",
    "    ]\n",
    ")\n",
    "\n",
    "meta_datasets[\"mix\"] = [\n",
    "    utils.gp_datasets.gp_dataset_generator(\n",
    "        kernel=kernels[i % 4], x_min=x_min, x_max=x_max\n",
    "    )\n",
    "    if i % 4 != 3\n",
    "    else utils.dataset_utils.sawtooth_dataset(lower=x_min, upper=x_max)\n",
    "    for i in range(num_datasets)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datasets = {}\n",
    "\n",
    "for kernel in kernels:\n",
    "    test_datasets[kernel] = utils.gp_datasets.gp_dataset_generator(\n",
    "        min_n=25, max_n=35, kernel=kernel\n",
    "    )\n",
    "\n",
    "test_datasets[\"saw\"] = utils.dataset_utils.sawtooth_dataset(min_n=50, max_n=60)\n",
    "\n",
    "test_datasets[\"cub\"] = utils.dataset_utils.cubic_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_metrics = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Amortised GIBNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amortised_gibnn = gibnn.AmortisedGIBNN(\n",
    "    x_dim=1,\n",
    "    y_dim=1,\n",
    "    hidden_dims=[50, 50],\n",
    "    in_hidden_dims=[50, 50],\n",
    "    noise=1e-1,\n",
    "    train_noise=False,\n",
    ")\n",
    "\n",
    "agibnn_tracker = utils.training_utils.train_metamodel(\n",
    "    amortised_gibnn,\n",
    "    meta_datasets[\"mix\"],\n",
    "    min_es_iters=2_000,\n",
    "    smooth_es_iters=500,\n",
    "    batch_size=1,\n",
    ")\n",
    "\n",
    "if plot_training_metrics:\n",
    "    fig, axes = plt.subplots(\n",
    "        len(agibnn_tracker.keys()),\n",
    "        1,\n",
    "        figsize=(8, len(agibnn_tracker.keys()) * 4),\n",
    "        dpi=100,\n",
    "        sharex=True,\n",
    "    )\n",
    "\n",
    "    for ax, (key, vals) in zip(axes, agibnn_tracker.items()):\n",
    "        ax.plot(vals)\n",
    "        ax.set_ylabel(key)\n",
    "        ax.grid()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amortised GIBNN with NP loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_amortised_gibnn = gibnn.AmortisedGIBNN(\n",
    "    x_dim=1,\n",
    "    y_dim=1,\n",
    "    hidden_dims=[50, 50],\n",
    "    in_hidden_dims=[50, 50],\n",
    "    noise=5e-2,\n",
    "    train_noise=False,\n",
    ")\n",
    "\n",
    "npagibnn_tracker = utils.training_utils.train_metamodel(\n",
    "    np_amortised_gibnn,\n",
    "    meta_datasets[\"mix\"],\n",
    "    np_loss=True,\n",
    "    min_es_iters=2_000,\n",
    "    smooth_es_iters=500,\n",
    "    batch_size=1,\n",
    ")\n",
    "\n",
    "if True:\n",
    "    fig, axes = plt.subplots(\n",
    "        len(npagibnn_tracker.keys()),\n",
    "        1,\n",
    "        figsize=(8, len(npagibnn_tracker.keys()) * 4),\n",
    "        dpi=100,\n",
    "        sharex=True,\n",
    "    )\n",
    "\n",
    "    for ax, (key, vals) in zip(axes, npagibnn_tracker.items()):\n",
    "        ax.plot(vals)\n",
    "        ax.set_ylabel(key)\n",
    "        ax.grid()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Amortised MFVI BNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amortised_mfvibnn = mfvi_bnn.AmortisedMFVIBNN(\n",
    "    x_dim=1,\n",
    "    y_dim=1,\n",
    "    hidden_dims=[20, 20],\n",
    "    in_hidden_dims=[20, 20],\n",
    "    noise=1e-2,\n",
    "    train_noise=True,\n",
    ")\n",
    "\n",
    "agibnn_tracker = utils.training_utils.train_metamodel(\n",
    "    amortised_mfvibnn,\n",
    "    meta_datasets[\"mix\"],\n",
    ")\n",
    "\n",
    "if plot_training_metrics:\n",
    "    fig, axes = plt.subplots(\n",
    "        len(agibnn_tracker.keys()),\n",
    "        1,\n",
    "        figsize=(8, len(agibnn_tracker.keys()) * 4),\n",
    "        dpi=100,\n",
    "        sharex=True,\n",
    "    )\n",
    "\n",
    "    for ax, (key, vals) in zip(axes, agibnn_tracker.items()):\n",
    "        ax.plot(vals)\n",
    "        ax.set_ylabel(key)\n",
    "        ax.grid()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Neural Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnp = np.CNP(\n",
    "    x_dim=1,\n",
    "    y_dim=1,\n",
    "    embedded_dim=64,\n",
    "    encoder_hidden_dims=[128, 128],\n",
    "    decoder_hidden_dims=[128, 128],\n",
    "    train_noise=True,\n",
    ")\n",
    "\n",
    "cnp_tracker = utils.training_utils.train_metamodel(\n",
    "    cnp,\n",
    "    meta_datasets[\"mix\"],\n",
    "    neural_process=True,\n",
    "    lr=1e-3,\n",
    "    max_iters=50_000,\n",
    "    batch_size=10,\n",
    "    min_es_iters=10_000,\n",
    "    ref_es_iters=1_000,\n",
    "    smooth_es_iters=500,\n",
    ")\n",
    "\n",
    "if plot_training_metrics:\n",
    "    fig, axes = plt.subplots(\n",
    "        len(cnp_tracker.keys()),\n",
    "        1,\n",
    "        figsize=(8, len(cnp_tracker.keys()) * 4),\n",
    "        dpi=100,\n",
    "        sharex=True,\n",
    "    )\n",
    "\n",
    "    for ax, (key, vals) in zip(axes, cnp_tracker.items()):\n",
    "        ax.plot(vals)\n",
    "        ax.set_ylabel(key)\n",
    "        ax.grid()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SE GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "x, y = test_datasets[\"se\"]\n",
    "dataset = torch.utils.data.TensorDataset(x, y)\n",
    "se_gp_model = gp.GPModel(x.squeeze(), y.squeeze(), likelihood, kernel=\"se\")\n",
    "\n",
    "se_gp_tracker = utils.training_utils.train_gp(\n",
    "    se_gp_model,\n",
    "    likelihood,\n",
    "    dataset,\n",
    "    dataset_size=x.shape[0],\n",
    ")\n",
    "\n",
    "if plot_training_metrics:\n",
    "    plt.plot(se_gp_tracker[\"loss\"])\n",
    "    plt.ylabel(\"marginal likelihood\")\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Periodic GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "x, y = test_datasets[\"per\"]\n",
    "dataset = torch.utils.data.TensorDataset(x, y)\n",
    "per_gp_model = gp.GPModel(x.squeeze(), y.squeeze(), likelihood, kernel=\"per\")\n",
    "\n",
    "per_gp_tracker = utils.training_utils.train_gp(\n",
    "    per_gp_model,\n",
    "    likelihood,\n",
    "    dataset,\n",
    "    dataset_size=x.shape[0],\n",
    ")\n",
    "\n",
    "if plot_training_metrics:\n",
    "    plt.plot(per_gp_tracker[\"loss\"])\n",
    "    plt.ylabel(\"marginal likelihood\")\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Laplacian GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "x, y = test_datasets[\"lap\"]\n",
    "dataset = torch.utils.data.TensorDataset(x, y)\n",
    "lap_gp_model = gp.GPModel(x.squeeze(), y.squeeze(), likelihood, kernel=\"lap\")\n",
    "\n",
    "likelihood.noise_covar.noise = 1e-2\n",
    "\n",
    "lap_gp_tracker = utils.training_utils.train_gp(\n",
    "    lap_gp_model,\n",
    "    likelihood,\n",
    "    dataset,\n",
    "    dataset_size=x.shape[0],\n",
    ")\n",
    "\n",
    "if plot_training_metrics:\n",
    "    plt.plot(lap_gp_tracker[\"loss\"])\n",
    "    plt.ylabel(\"marginal likelihood\")\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Periodic GP for sawtooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "x, y = test_datasets[\"saw\"]\n",
    "dataset = torch.utils.data.TensorDataset(x, y)\n",
    "saw_gp_model = gp.GPModel(x.squeeze(), y.squeeze(), likelihood, kernel=\"saw\")\n",
    "\n",
    "saw_gp_tracker = utils.training_utils.train_gp(\n",
    "    saw_gp_model,\n",
    "    likelihood,\n",
    "    dataset,\n",
    "    dataset_size=x.shape[0],\n",
    ")\n",
    "\n",
    "if plot_training_metrics:\n",
    "    plt.plot(saw_gp_tracker[\"loss\"])\n",
    "    plt.ylabel(\"marginal likelihood\")\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GIBNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = test_datasets[\"cub\"]\n",
    "dataset = torch.utils.data.TensorDataset(x, y)\n",
    "n = x.shape[0]\n",
    "num_inducing = 10\n",
    "rand_perm = torch.randperm(n)[:num_inducing]\n",
    "inducing_points = x[rand_perm]\n",
    "\n",
    "v_gibnn = gibnn.GIBNN(\n",
    "    x_dim=1,\n",
    "    y_dim=1,\n",
    "    hidden_dims=[20, 20],\n",
    "    num_inducing=num_inducing,\n",
    "    inducing_points=inducing_points,\n",
    "    train_noise=True,\n",
    ")\n",
    "\n",
    "v_gibnn_tracker = utils.training_utils.train_model(\n",
    "    v_gibnn,\n",
    "    dataset,\n",
    "    batch_size=128,\n",
    "    lr=1e-2,\n",
    ")\n",
    "\n",
    "if plot_training_metrics:\n",
    "    fig, axes = plt.subplots(\n",
    "        len(v_gibnn_tracker.keys()),\n",
    "        1,\n",
    "        figsize=(8, len(v_gibnn_tracker.keys()) * 4),\n",
    "        dpi=100,\n",
    "        sharex=True,\n",
    "    )\n",
    "\n",
    "    for ax, (key, vals) in zip(axes, v_gibnn_tracker.items()):\n",
    "        ax.plot(vals)\n",
    "        ax.set_ylabel(key)\n",
    "        ax.grid()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = torch.linspace(-5.0, 5.0, 1000).unsqueeze(-1)\n",
    "num_models = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SE covariance GP-generated test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(num_models, 1, figsize=(6, 3 * num_models), sharex=True)\n",
    "x, y = test_datasets[\"se\"]\n",
    "\n",
    "# generate predictions\n",
    "agibnn_preds = amortised_gibnn(x, y, x_test=x_test, num_samples=100)[-1]\n",
    "npagibnn_preds = np_amortised_gibnn(x, y, x_test=x_test, num_samples=100)[-1]\n",
    "amfvibnn_preds = amortised_mfvibnn(x, y, x_test=x_test, num_samples=100)[-1]\n",
    "cnp_preds = cnp(x, y, x_t=x_test)\n",
    "se_gp_model.eval()\n",
    "se_gp_preds = se_gp_model(x_test)\n",
    "\n",
    "# AGIBNN\n",
    "axes[0].plot(\n",
    "    x_test,\n",
    "    agibnn_preds[-1].detach().numpy(),\n",
    "    color=\"C0\",\n",
    "    alpha=0.1,\n",
    "    zorder=0,\n",
    "    label=\"Prediction samples\",\n",
    ")\n",
    "for pred in agibnn_preds[:-1]:\n",
    "    axes[0].plot(x_test, pred.detach().numpy(), color=\"C0\", alpha=0.1, zorder=0)\n",
    "axes[0].plot(\n",
    "    x_test,\n",
    "    agibnn_preds.detach().mean(0).numpy(),\n",
    "    color=\"C0\",\n",
    "    alpha=1.0,\n",
    "    ls=\"--\",\n",
    "    zorder=0,\n",
    "    label=\"Mean prediction\",\n",
    ")\n",
    "# axes[0].fill_between([x_min, 3], 5, -5, color=\"grey\", alpha=0.2)\n",
    "axes[0].scatter(x, y, color=\"C1\", label=\"Datapoints\", zorder=1, s=10)\n",
    "axes[0].set_title(\"Amortised GIBNN\", fontsize=18)\n",
    "axes[0].set_ylim([-5.0, 5.0])\n",
    "axes[0].set_xlim([-5.0, 5.0])\n",
    "axes[0].set_xticklabels([])\n",
    "axes[0].set_xticks([], [])\n",
    "axes[0].set_yticklabels([])\n",
    "axes[0].set_yticks([], [])\n",
    "# axes[0].legend()\n",
    "\n",
    "# MFVIBNN\n",
    "axes[1].plot(\n",
    "    x_test,\n",
    "    amfvibnn_preds[-1].detach().numpy(),\n",
    "    color=\"C0\",\n",
    "    alpha=0.1,\n",
    "    zorder=0,\n",
    "    label=\"Prediction samples\",\n",
    ")\n",
    "for pred in amfvibnn_preds[:-1]:\n",
    "    axes[1].plot(x_test, pred.detach().numpy(), color=\"C0\", alpha=0.1, zorder=0)\n",
    "axes[1].plot(\n",
    "    x_test,\n",
    "    amfvibnn_preds.detach().mean(0).numpy(),\n",
    "    color=\"C0\",\n",
    "    alpha=1.0,\n",
    "    ls=\"--\",\n",
    "    zorder=0,\n",
    "    label=\"Mean prediction\",\n",
    ")\n",
    "# axes[1].fill_between([x_min, 3], 5, -5, color=\"grey\", alpha=0.2)\n",
    "axes[1].scatter(x, y, color=\"C1\", label=\"Datapoints\", zorder=1, s=10)\n",
    "axes[1].set_title(\"Amortised MFVIBNN\", fontsize=18)\n",
    "axes[1].set_ylim([-5.0, 5.0])\n",
    "axes[1].set_xlim([-5.0, 5.0])\n",
    "axes[1].set_xticklabels([])\n",
    "axes[1].set_xticks([], [])\n",
    "axes[1].set_yticklabels([])\n",
    "axes[1].set_yticks([], [])\n",
    "# axes[1].legend()\n",
    "\n",
    "# CNP\n",
    "pred_mean = cnp_preds.loc.detach().numpy()\n",
    "pred_std = cnp_preds.scale.detach().numpy()\n",
    "axes[2].plot(\n",
    "    x_test,\n",
    "    pred_mean,\n",
    "    color=\"C0\",\n",
    "    alpha=1.0,\n",
    "    ls=\"--\",\n",
    "    zorder=0,\n",
    "    label=\"Mean rediction\",\n",
    ")\n",
    "axes[2].fill_between(\n",
    "    x_test.squeeze(),\n",
    "    (pred_mean + 2 * pred_std).squeeze(),\n",
    "    (pred_mean - 2 * pred_std).squeeze(),\n",
    "    # color=\"C0\",\n",
    "    alpha=0.3,\n",
    "    label=\"95% confidence interval\",\n",
    ")\n",
    "# axes[2].fill_between([x_min, 3], 5, -5, color=\"grey\", alpha=0.2)\n",
    "axes[2].scatter(x, y, color=\"C1\", label=\"Datapoints\", zorder=1, s=10)\n",
    "axes[2].set_title(\"Vanilla CNP\", fontsize=18)\n",
    "axes[2].set_ylim([-5.0, 5.0])\n",
    "axes[2].set_xlim([-5.0, 5.0])\n",
    "axes[2].set_xticklabels([])\n",
    "axes[2].set_xticks([], [])\n",
    "axes[2].set_yticklabels([])\n",
    "axes[2].set_yticks([], [])\n",
    "# axes[2].legend()\n",
    "\n",
    "# GP\n",
    "pred_mean = se_gp_preds.mean.detach().numpy()\n",
    "pred_std = torch.sqrt(se_gp_preds.variance.detach()).numpy()\n",
    "axes[3].plot(\n",
    "    x_test,\n",
    "    pred_mean,\n",
    "    color=\"C0\",\n",
    "    alpha=1.0,\n",
    "    ls=\"--\",\n",
    "    zorder=0,\n",
    "    label=\"Mean prediction\",\n",
    ")\n",
    "axes[3].fill_between(\n",
    "    x_test.squeeze(),\n",
    "    (pred_mean + 2 * pred_std).squeeze(),\n",
    "    (pred_mean - 2 * pred_std).squeeze(),\n",
    "    # color=\"C0\",\n",
    "    alpha=0.3,\n",
    "    label=\"95% confidence interval\",\n",
    ")\n",
    "axes[3].scatter(x, y, color=\"C1\", label=\"Datapoints\", zorder=1, s=10)\n",
    "axes[3].set_title(\"SE covariance GP\", fontsize=18)\n",
    "axes[3].set_ylim([-5.0, 5.0])\n",
    "axes[3].set_xlim([-5.0, 5.0])\n",
    "axes[3].set_xticklabels([])\n",
    "axes[3].set_xticks([], [])\n",
    "axes[3].set_yticklabels([])\n",
    "axes[3].set_yticks([], [])\n",
    "# axes[3].legend()\n",
    "\n",
    "# NP loss AGIBNN\n",
    "axes[4].plot(\n",
    "    x_test,\n",
    "    npagibnn_preds[-1].detach().numpy(),\n",
    "    color=\"C0\",\n",
    "    alpha=0.1,\n",
    "    zorder=0,\n",
    "    label=\"Prediction samples\",\n",
    ")\n",
    "for pred in npagibnn_preds[:-1]:\n",
    "    axes[4].plot(x_test, pred.detach().numpy(), color=\"C0\", alpha=0.1, zorder=0)\n",
    "axes[4].plot(\n",
    "    x_test,\n",
    "    npagibnn_preds.detach().mean(0).numpy(),\n",
    "    color=\"C0\",\n",
    "    alpha=1.0,\n",
    "    ls=\"--\",\n",
    "    zorder=0,\n",
    "    label=\"Mean prediction\",\n",
    ")\n",
    "# axes[4].fill_between([x_min, 3], 5, -5, color=\"grey\", alpha=0.2)\n",
    "axes[4].scatter(x, y, color=\"C1\", label=\"Datapoints\", zorder=1, s=10)\n",
    "axes[4].set_title(\"Amortised GIBNN, NP Loss\", fontsize=18)\n",
    "axes[4].set_ylim([-5.0, 5.0])\n",
    "axes[4].set_xlim([-5.0, 5.0])\n",
    "axes[4].set_xticklabels([])\n",
    "axes[4].set_xticks([], [])\n",
    "axes[4].set_yticklabels([])\n",
    "axes[4].set_yticks([], [])\n",
    "# axes[4].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Periodic covariance GP-generated test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(num_models, 1, figsize=(6, 3 * num_models), sharex=True)\n",
    "x, y = test_datasets[\"per\"]\n",
    "\n",
    "# generate predictions\n",
    "agibnn_preds = amortised_gibnn(x, y, x_test=x_test, num_samples=100)[-1]\n",
    "npagibnn_preds = np_amortised_gibnn(x, y, x_test=x_test, num_samples=100)[-1]\n",
    "amfvibnn_preds = amortised_mfvibnn(x, y, x_test=x_test, num_samples=100)[-1]\n",
    "cnp_preds = cnp(x, y, x_t=x_test)\n",
    "per_gp_model.eval()\n",
    "per_gp_preds = per_gp_model(x_test)\n",
    "\n",
    "# AGIBNN\n",
    "axes[0].plot(\n",
    "    x_test,\n",
    "    agibnn_preds[-1].detach().numpy(),\n",
    "    color=\"C0\",\n",
    "    alpha=0.1,\n",
    "    zorder=0,\n",
    "    label=\"Prediction samples\",\n",
    ")\n",
    "for pred in agibnn_preds[:-1]:\n",
    "    axes[0].plot(x_test, pred.detach().numpy(), color=\"C0\", alpha=0.1, zorder=0)\n",
    "axes[0].plot(\n",
    "    x_test,\n",
    "    agibnn_preds.detach().mean(0).numpy(),\n",
    "    color=\"C0\",\n",
    "    alpha=1.0,\n",
    "    ls=\"--\",\n",
    "    zorder=0,\n",
    "    label=\"Mean prediction\",\n",
    ")\n",
    "# axes[0].fill_between([x_min, 3], 5, -5, color=\"grey\", alpha=0.2)\n",
    "axes[0].scatter(x, y, color=\"C1\", label=\"Datapoints\", zorder=1, s=10)\n",
    "axes[0].set_title(\"Amortised GIBNN\", fontsize=18)\n",
    "axes[0].set_ylim([-5.0, 5.0])\n",
    "axes[0].set_xlim([-5.0, 5.0])\n",
    "axes[0].set_xticklabels([])\n",
    "axes[0].set_xticks([], [])\n",
    "axes[0].set_yticklabels([])\n",
    "axes[0].set_yticks([], [])\n",
    "# axes[0].legend()\n",
    "\n",
    "# MFVIBNN\n",
    "axes[1].plot(\n",
    "    x_test,\n",
    "    amfvibnn_preds[-1].detach().numpy(),\n",
    "    color=\"C0\",\n",
    "    alpha=0.1,\n",
    "    zorder=0,\n",
    "    label=\"Prediction samples\",\n",
    ")\n",
    "for pred in amfvibnn_preds[:-1]:\n",
    "    axes[1].plot(x_test, pred.detach().numpy(), color=\"C0\", alpha=0.1, zorder=0)\n",
    "axes[1].plot(\n",
    "    x_test,\n",
    "    amfvibnn_preds.detach().mean(0).numpy(),\n",
    "    color=\"C0\",\n",
    "    alpha=1.0,\n",
    "    ls=\"--\",\n",
    "    zorder=0,\n",
    "    label=\"Mean prediction\",\n",
    ")\n",
    "# axes[1].fill_between([x_min, 3], 5, -5, color=\"grey\", alpha=0.2)\n",
    "axes[1].scatter(x, y, color=\"C1\", label=\"Datapoints\", zorder=1, s=10)\n",
    "axes[1].set_title(\"Amortised MFVIBNN\", fontsize=18)\n",
    "axes[1].set_ylim([-5.0, 5.0])\n",
    "axes[1].set_xlim([-5.0, 5.0])\n",
    "axes[1].set_xticklabels([])\n",
    "axes[1].set_xticks([], [])\n",
    "axes[1].set_yticklabels([])\n",
    "axes[1].set_yticks([], [])\n",
    "# axes[1].legend()\n",
    "\n",
    "# CNP\n",
    "pred_mean = cnp_preds.loc.detach().numpy()\n",
    "pred_std = cnp_preds.scale.detach().numpy()\n",
    "axes[2].plot(\n",
    "    x_test,\n",
    "    pred_mean,\n",
    "    color=\"C0\",\n",
    "    alpha=1.0,\n",
    "    ls=\"--\",\n",
    "    zorder=0,\n",
    "    label=\"Mean rediction\",\n",
    ")\n",
    "axes[2].fill_between(\n",
    "    x_test.squeeze(),\n",
    "    (pred_mean + 2 * pred_std).squeeze(),\n",
    "    (pred_mean - 2 * pred_std).squeeze(),\n",
    "    # color=\"C0\",\n",
    "    alpha=0.3,\n",
    "    label=\"95% confidence interval\",\n",
    ")\n",
    "# axes[2].fill_between([x_min, 3], 5, -5, color=\"grey\", alpha=0.2)\n",
    "axes[2].scatter(x, y, color=\"C1\", label=\"Datapoints\", zorder=1, s=10)\n",
    "axes[2].set_title(\"Vanilla CNP\", fontsize=18)\n",
    "axes[2].set_ylim([-5.0, 5.0])\n",
    "axes[2].set_xlim([-5.0, 5.0])\n",
    "axes[2].set_xticklabels([])\n",
    "axes[2].set_xticks([], [])\n",
    "axes[2].set_yticklabels([])\n",
    "axes[2].set_yticks([], [])\n",
    "# axes[2].legend()\n",
    "\n",
    "# GP\n",
    "pred_mean = per_gp_preds.mean.detach().numpy()\n",
    "pred_std = torch.sqrt(per_gp_preds.variance.detach()).numpy()\n",
    "axes[3].plot(\n",
    "    x_test,\n",
    "    pred_mean,\n",
    "    color=\"C0\",\n",
    "    alpha=1.0,\n",
    "    ls=\"--\",\n",
    "    zorder=0,\n",
    "    label=\"Mean prediction\",\n",
    ")\n",
    "axes[3].fill_between(\n",
    "    x_test.squeeze(),\n",
    "    (pred_mean + 2 * pred_std).squeeze(),\n",
    "    (pred_mean - 2 * pred_std).squeeze(),\n",
    "    # color=\"C0\",\n",
    "    alpha=0.3,\n",
    "    label=\"95% confidence interval\",\n",
    ")\n",
    "axes[3].scatter(x, y, color=\"C1\", label=\"Datapoints\", zorder=1, s=10)\n",
    "axes[3].set_title(\"Periodic covariance GP\", fontsize=18)\n",
    "axes[3].set_ylim([-5.0, 5.0])\n",
    "axes[3].set_xlim([-5.0, 5.0])\n",
    "axes[3].set_xticklabels([])\n",
    "axes[3].set_xticks([], [])\n",
    "axes[3].set_yticklabels([])\n",
    "axes[3].set_yticks([], [])\n",
    "# axes[3].legend()\n",
    "\n",
    "# NP loss AGIBNN\n",
    "axes[4].plot(\n",
    "    x_test,\n",
    "    npagibnn_preds[-1].detach().numpy(),\n",
    "    color=\"C0\",\n",
    "    alpha=0.1,\n",
    "    zorder=0,\n",
    "    label=\"Prediction samples\",\n",
    ")\n",
    "for pred in npagibnn_preds[:-1]:\n",
    "    axes[4].plot(x_test, pred.detach().numpy(), color=\"C0\", alpha=0.1, zorder=0)\n",
    "axes[4].plot(\n",
    "    x_test,\n",
    "    npagibnn_preds.detach().mean(0).numpy(),\n",
    "    color=\"C0\",\n",
    "    alpha=1.0,\n",
    "    ls=\"--\",\n",
    "    zorder=0,\n",
    "    label=\"Mean prediction\",\n",
    ")\n",
    "# axes[4].fill_between([x_min, 3], 5, -5, color=\"grey\", alpha=0.2)\n",
    "axes[4].scatter(x, y, color=\"C1\", label=\"Datapoints\", zorder=1, s=10)\n",
    "axes[4].set_title(\"Amortised GIBNN, NP Loss\", fontsize=18)\n",
    "axes[4].set_ylim([-5.0, 5.0])\n",
    "axes[4].set_xlim([-5.0, 5.0])\n",
    "axes[4].set_xticklabels([])\n",
    "axes[4].set_xticks([], [])\n",
    "axes[4].set_yticklabels([])\n",
    "axes[4].set_yticks([], [])\n",
    "# axes[4].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Laplacian covariance GP-generated test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(num_models, 1, figsize=(6, 3 * num_models), sharex=True)\n",
    "x, y = test_datasets[\"lap\"]\n",
    "\n",
    "# generate predictions\n",
    "agibnn_preds = amortised_gibnn(x, y, x_test=x_test, num_samples=100)[-1]\n",
    "npagibnn_preds = np_amortised_gibnn(x, y, x_test=x_test, num_samples=100)[-1]\n",
    "amfvibnn_preds = amortised_mfvibnn(x, y, x_test=x_test, num_samples=100)[-1]\n",
    "cnp_preds = cnp(x, y, x_t=x_test)\n",
    "lap_gp_model.eval()\n",
    "lap_gp_preds = lap_gp_model(x_test)\n",
    "\n",
    "# AGIBNN\n",
    "axes[0].plot(\n",
    "    x_test,\n",
    "    agibnn_preds[-1].detach().numpy(),\n",
    "    color=\"C0\",\n",
    "    alpha=0.1,\n",
    "    zorder=0,\n",
    "    label=\"Prediction samples\",\n",
    ")\n",
    "for pred in agibnn_preds[:-1]:\n",
    "    axes[0].plot(x_test, pred.detach().numpy(), color=\"C0\", alpha=0.1, zorder=0)\n",
    "axes[0].plot(\n",
    "    x_test,\n",
    "    agibnn_preds.detach().mean(0).numpy(),\n",
    "    color=\"C0\",\n",
    "    alpha=1.0,\n",
    "    ls=\"--\",\n",
    "    zorder=0,\n",
    "    label=\"Mean prediction\",\n",
    ")\n",
    "# axes[0].fill_between([x_min, 3], 5, -5, color=\"grey\", alpha=0.2)\n",
    "axes[0].scatter(x, y, color=\"C1\", label=\"Datapoints\", zorder=1, s=10)\n",
    "axes[0].set_title(\"Amortised GIBNN\", fontsize=18)\n",
    "axes[0].set_ylim([-5.0, 5.0])\n",
    "axes[0].set_xlim([-5.0, 5.0])\n",
    "axes[0].set_xticklabels([])\n",
    "axes[0].set_xticks([], [])\n",
    "axes[0].set_yticklabels([])\n",
    "axes[0].set_yticks([], [])\n",
    "# axes[0].legend()\n",
    "\n",
    "# MFVIBNN\n",
    "axes[1].plot(\n",
    "    x_test,\n",
    "    amfvibnn_preds[-1].detach().numpy(),\n",
    "    color=\"C0\",\n",
    "    alpha=0.1,\n",
    "    zorder=0,\n",
    "    label=\"Prediction samples\",\n",
    ")\n",
    "for pred in amfvibnn_preds[:-1]:\n",
    "    axes[1].plot(x_test, pred.detach().numpy(), color=\"C0\", alpha=0.1, zorder=0)\n",
    "axes[1].plot(\n",
    "    x_test,\n",
    "    amfvibnn_preds.detach().mean(0).numpy(),\n",
    "    color=\"C0\",\n",
    "    alpha=1.0,\n",
    "    ls=\"--\",\n",
    "    zorder=0,\n",
    "    label=\"Mean prediction\",\n",
    ")\n",
    "# axes[1].fill_between([x_min, 3], 5, -5, color=\"grey\", alpha=0.2)\n",
    "axes[1].scatter(x, y, color=\"C1\", label=\"Datapoints\", zorder=1, s=10)\n",
    "axes[1].set_title(\"Amortised MFVIBNN\", fontsize=18)\n",
    "axes[1].set_ylim([-5.0, 5.0])\n",
    "axes[1].set_xlim([-5.0, 5.0])\n",
    "axes[1].set_xticklabels([])\n",
    "axes[1].set_xticks([], [])\n",
    "axes[1].set_yticklabels([])\n",
    "axes[1].set_yticks([], [])\n",
    "# axes[1].legend()\n",
    "\n",
    "# CNP\n",
    "pred_mean = cnp_preds.loc.detach().numpy()\n",
    "pred_std = cnp_preds.scale.detach().numpy()\n",
    "axes[2].plot(\n",
    "    x_test,\n",
    "    pred_mean,\n",
    "    color=\"C0\",\n",
    "    alpha=1.0,\n",
    "    ls=\"--\",\n",
    "    zorder=0,\n",
    "    label=\"Mean rediction\",\n",
    ")\n",
    "axes[2].fill_between(\n",
    "    x_test.squeeze(),\n",
    "    (pred_mean + 2 * pred_std).squeeze(),\n",
    "    (pred_mean - 2 * pred_std).squeeze(),\n",
    "    # color=\"C0\",\n",
    "    alpha=0.3,\n",
    "    label=\"95% confidence interval\",\n",
    ")\n",
    "# axes[2].fill_between([x_min, 3], 5, -5, color=\"grey\", alpha=0.2)\n",
    "axes[2].scatter(x, y, color=\"C1\", label=\"Datapoints\", zorder=1, s=10)\n",
    "axes[2].set_title(\"Vanilla CNP\", fontsize=18)\n",
    "axes[2].set_ylim([-5.0, 5.0])\n",
    "axes[2].set_xlim([-5.0, 5.0])\n",
    "axes[2].set_xticklabels([])\n",
    "axes[2].set_xticks([], [])\n",
    "axes[2].set_yticklabels([])\n",
    "axes[2].set_yticks([], [])\n",
    "# axes[2].legend()\n",
    "\n",
    "# GP\n",
    "pred_mean = lap_gp_preds.mean.detach().numpy()\n",
    "pred_std = torch.sqrt(lap_gp_preds.variance.detach()).numpy()\n",
    "axes[3].plot(\n",
    "    x_test,\n",
    "    pred_mean,\n",
    "    color=\"C0\",\n",
    "    alpha=1.0,\n",
    "    ls=\"--\",\n",
    "    zorder=0,\n",
    "    label=\"Mean prediction\",\n",
    ")\n",
    "axes[3].fill_between(\n",
    "    x_test.squeeze(),\n",
    "    (pred_mean + 2 * pred_std).squeeze(),\n",
    "    (pred_mean - 2 * pred_std).squeeze(),\n",
    "    # color=\"C0\",\n",
    "    alpha=0.3,\n",
    "    label=\"95% confidence interval\",\n",
    ")\n",
    "axes[3].scatter(x, y, color=\"C1\", label=\"Datapoints\", zorder=1, s=10)\n",
    "axes[3].set_title(\"Laplacian covariance GP\", fontsize=18)\n",
    "axes[3].set_ylim([-5.0, 5.0])\n",
    "axes[3].set_xlim([-5.0, 5.0])\n",
    "axes[3].set_xticklabels([])\n",
    "axes[3].set_xticks([], [])\n",
    "axes[3].set_yticklabels([])\n",
    "axes[3].set_yticks([], [])\n",
    "# axes[3].legend()\n",
    "\n",
    "# NP loss AGIBNN\n",
    "axes[4].plot(\n",
    "    x_test,\n",
    "    npagibnn_preds[-1].detach().numpy(),\n",
    "    color=\"C0\",\n",
    "    alpha=0.1,\n",
    "    zorder=0,\n",
    "    label=\"Prediction samples\",\n",
    ")\n",
    "for pred in npagibnn_preds[:-1]:\n",
    "    axes[4].plot(x_test, pred.detach().numpy(), color=\"C0\", alpha=0.1, zorder=0)\n",
    "axes[4].plot(\n",
    "    x_test,\n",
    "    npagibnn_preds.detach().mean(0).numpy(),\n",
    "    color=\"C0\",\n",
    "    alpha=1.0,\n",
    "    ls=\"--\",\n",
    "    zorder=0,\n",
    "    label=\"Mean prediction\",\n",
    ")\n",
    "# axes[4].fill_between([x_min, 3], 5, -5, color=\"grey\", alpha=0.2)\n",
    "axes[4].scatter(x, y, color=\"C1\", label=\"Datapoints\", zorder=1, s=10)\n",
    "axes[4].set_title(\"Amortised GIBNN, NP Loss\", fontsize=18)\n",
    "axes[4].set_ylim([-5.0, 5.0])\n",
    "axes[4].set_xlim([-5.0, 5.0])\n",
    "axes[4].set_xticklabels([])\n",
    "axes[4].set_xticks([], [])\n",
    "axes[4].set_yticklabels([])\n",
    "axes[4].set_yticks([], [])\n",
    "# axes[4].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sawtooth dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(num_models, 1, figsize=(6, 3 * num_models), sharex=True)\n",
    "x, y = test_datasets[\"saw\"]\n",
    "\n",
    "# generate predictions\n",
    "agibnn_preds = amortised_gibnn(x, y, x_test=x_test, num_samples=100)[-1]\n",
    "npagibnn_preds = np_amortised_gibnn(x, y, x_test=x_test, num_samples=100)[-1]\n",
    "amfvibnn_preds = amortised_mfvibnn(x, y, x_test=x_test, num_samples=100)[-1]\n",
    "cnp_preds = cnp(x, y, x_t=x_test)\n",
    "saw_gp_model.eval()\n",
    "saw_gp_preds = saw_gp_model(x_test)\n",
    "\n",
    "# AGIBNN\n",
    "axes[0].plot(\n",
    "    x_test,\n",
    "    agibnn_preds[-1].detach().numpy(),\n",
    "    color=\"C0\",\n",
    "    alpha=0.1,\n",
    "    zorder=0,\n",
    "    label=\"Prediction samples\",\n",
    ")\n",
    "for pred in agibnn_preds[:-1]:\n",
    "    axes[0].plot(x_test, pred.detach().numpy(), color=\"C0\", alpha=0.1, zorder=0)\n",
    "axes[0].plot(\n",
    "    x_test,\n",
    "    agibnn_preds.detach().mean(0).numpy(),\n",
    "    color=\"C0\",\n",
    "    alpha=1.0,\n",
    "    ls=\"--\",\n",
    "    zorder=0,\n",
    "    label=\"Mean prediction\",\n",
    ")\n",
    "# axes[0].fill_between([x_min, 3], 5, -5, color=\"grey\", alpha=0.2)\n",
    "axes[0].scatter(x, y, color=\"C1\", label=\"Datapoints\", zorder=1, s=10)\n",
    "axes[0].set_title(\"Amortised GIBNN\")\n",
    "axes[0].set_ylim([-5.0, 5.0])\n",
    "axes[0].set_xlim([-5.0, 5.0])\n",
    "axes[0].set_xticklabels([])\n",
    "axes[0].set_xticks([], [])\n",
    "axes[0].set_yticklabels([])\n",
    "axes[0].set_yticks([], [])\n",
    "# axes[0].legend()\n",
    "\n",
    "# MFVIBNN\n",
    "axes[1].plot(\n",
    "    x_test,\n",
    "    amfvibnn_preds[-1].detach().numpy(),\n",
    "    color=\"C0\",\n",
    "    alpha=0.1,\n",
    "    zorder=0,\n",
    "    label=\"Prediction samples\",\n",
    ")\n",
    "for pred in amfvibnn_preds[:-1]:\n",
    "    axes[1].plot(x_test, pred.detach().numpy(), color=\"C0\", alpha=0.1, zorder=0)\n",
    "axes[1].plot(\n",
    "    x_test,\n",
    "    amfvibnn_preds.detach().mean(0).numpy(),\n",
    "    color=\"C0\",\n",
    "    alpha=1.0,\n",
    "    ls=\"--\",\n",
    "    zorder=0,\n",
    "    label=\"Mean prediction\",\n",
    ")\n",
    "# axes[1].fill_between([x_min, 3], 5, -5, color=\"grey\", alpha=0.2)\n",
    "axes[1].scatter(x, y, color=\"C1\", label=\"Datapoints\", zorder=1, s=10)\n",
    "axes[1].set_title(\"Amortised MFVIBNN\")\n",
    "axes[1].set_ylim([-5.0, 5.0])\n",
    "axes[1].set_xlim([-5.0, 5.0])\n",
    "axes[1].set_xticklabels([])\n",
    "axes[1].set_xticks([], [])\n",
    "axes[1].set_yticklabels([])\n",
    "axes[1].set_yticks([], [])\n",
    "# axes[1].legend()\n",
    "\n",
    "# CNP\n",
    "pred_mean = cnp_preds.loc.detach().numpy()\n",
    "pred_std = cnp_preds.scale.detach().numpy()\n",
    "axes[2].plot(\n",
    "    x_test,\n",
    "    pred_mean,\n",
    "    color=\"C0\",\n",
    "    alpha=1.0,\n",
    "    ls=\"--\",\n",
    "    zorder=0,\n",
    "    label=\"Mean rediction\",\n",
    ")\n",
    "axes[2].fill_between(\n",
    "    x_test.squeeze(),\n",
    "    (pred_mean + 2 * pred_std).squeeze(),\n",
    "    (pred_mean - 2 * pred_std).squeeze(),\n",
    "    # color=\"C0\",\n",
    "    alpha=0.3,\n",
    "    label=\"95% confidence interval\",\n",
    ")\n",
    "# axes[2].fill_between([x_min, 3], 5, -5, color=\"grey\", alpha=0.2)\n",
    "axes[2].scatter(x, y, color=\"C1\", label=\"Datapoints\", zorder=1, s=10)\n",
    "axes[2].set_title(\"Vanilla CNP\")\n",
    "axes[2].set_ylim([-5.0, 5.0])\n",
    "axes[2].set_xlim([-5.0, 5.0])\n",
    "axes[2].set_xticklabels([])\n",
    "axes[2].set_xticks([], [])\n",
    "axes[2].set_yticklabels([])\n",
    "axes[2].set_yticks([], [])\n",
    "# axes[2].legend()\n",
    "\n",
    "# GP\n",
    "pred_mean = saw_gp_preds.mean.detach().numpy()\n",
    "pred_std = torch.sqrt(saw_gp_preds.variance.detach()).numpy()\n",
    "axes[3].plot(\n",
    "    x_test,\n",
    "    pred_mean,\n",
    "    color=\"C0\",\n",
    "    alpha=1.0,\n",
    "    ls=\"--\",\n",
    "    zorder=0,\n",
    "    label=\"Mean prediction\",\n",
    ")\n",
    "axes[3].fill_between(\n",
    "    x_test.squeeze(),\n",
    "    (pred_mean + 2 * pred_std).squeeze(),\n",
    "    (pred_mean - 2 * pred_std).squeeze(),\n",
    "    # color=\"C0\",\n",
    "    alpha=0.3,\n",
    "    label=\"95% confidence interval\",\n",
    ")\n",
    "axes[3].scatter(x, y, color=\"C1\", label=\"Datapoints\", zorder=1, s=10)\n",
    "axes[3].set_title(\"Periodic covariance GP\")\n",
    "axes[3].set_ylim([-5.0, 5.0])\n",
    "axes[3].set_xlim([-5.0, 5.0])\n",
    "axes[3].set_xticklabels([])\n",
    "axes[3].set_xticks([], [])\n",
    "axes[3].set_yticklabels([])\n",
    "axes[3].set_yticks([], [])\n",
    "# axes[3].legend()\n",
    "\n",
    "# NP loss AGIBNN\n",
    "axes[4].plot(\n",
    "    x_test,\n",
    "    npagibnn_preds[-1].detach().numpy(),\n",
    "    color=\"C0\",\n",
    "    alpha=0.1,\n",
    "    zorder=0,\n",
    "    label=\"Prediction samples\",\n",
    ")\n",
    "for pred in npagibnn_preds[:-1]:\n",
    "    axes[4].plot(x_test, pred.detach().numpy(), color=\"C0\", alpha=0.1, zorder=0)\n",
    "axes[4].plot(\n",
    "    x_test,\n",
    "    npagibnn_preds.detach().mean(0).numpy(),\n",
    "    color=\"C0\",\n",
    "    alpha=1.0,\n",
    "    ls=\"--\",\n",
    "    zorder=0,\n",
    "    label=\"Mean prediction\",\n",
    ")\n",
    "# axes[4].fill_between([x_min, 3], 5, -5, color=\"grey\", alpha=0.2)\n",
    "axes[4].scatter(x, y, color=\"C1\", label=\"Datapoints\", zorder=1, s=10)\n",
    "axes[4].set_title(\"Amortised GIBNN, NP Loss\", fontsize=18)\n",
    "axes[4].set_ylim([-5.0, 5.0])\n",
    "axes[4].set_xlim([-5.0, 5.0])\n",
    "axes[4].set_xticklabels([])\n",
    "axes[4].set_xticks([], [])\n",
    "axes[4].set_yticklabels([])\n",
    "axes[4].set_yticks([], [])\n",
    "# axes[4].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cubic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(num_models, 1, figsize=(6, 3 * num_models), sharex=True)\n",
    "x, y = test_datasets[\"cub\"]\n",
    "\n",
    "# generate predictions\n",
    "agibnn_preds = amortised_gibnn(x, y, x_test=x_test, num_samples=100)[-1]\n",
    "npagibnn_preds = np_amortised_gibnn(x, y, x_test=x_test, num_samples=100)[-1]\n",
    "amfvibnn_preds = amortised_mfvibnn(x, y, x_test=x_test, num_samples=100)[-1]\n",
    "cnp_preds = cnp(x, y, x_t=x_test)\n",
    "v_gibnn_preds = v_gibnn(x_test, num_samples=100)[0]\n",
    "\n",
    "# AGIBNN\n",
    "axes[0].plot(\n",
    "    x_test,\n",
    "    agibnn_preds[-1].detach().numpy(),\n",
    "    color=\"C0\",\n",
    "    alpha=0.1,\n",
    "    zorder=0,\n",
    "    label=\"Prediction samples\",\n",
    ")\n",
    "for pred in agibnn_preds[:-1]:\n",
    "    axes[0].plot(x_test, pred.detach().numpy(), color=\"C0\", alpha=0.1, zorder=0)\n",
    "axes[0].plot(\n",
    "    x_test,\n",
    "    agibnn_preds.detach().mean(0).numpy(),\n",
    "    color=\"C0\",\n",
    "    alpha=1.0,\n",
    "    ls=\"--\",\n",
    "    zorder=0,\n",
    "    label=\"Mean prediction\",\n",
    ")\n",
    "# axes[0].fill_between([x_min, 3], 5, -5, color=\"grey\", alpha=0.2)\n",
    "axes[0].scatter(x, y, color=\"C1\", label=\"Datapoints\", zorder=1, s=10)\n",
    "axes[0].set_title(\"Amortised GIBNN\", fontsize=18)\n",
    "axes[0].set_ylim([-5.0, 5.0])\n",
    "axes[0].set_xlim([-5.0, 5.0])\n",
    "axes[0].set_xticklabels([])\n",
    "axes[0].set_xticks([], [])\n",
    "axes[0].set_yticklabels([])\n",
    "axes[0].set_yticks([], [])\n",
    "# axes[0].legend()\n",
    "\n",
    "# MFVIBNN\n",
    "axes[1].plot(\n",
    "    x_test,\n",
    "    amfvibnn_preds[-1].detach().numpy(),\n",
    "    color=\"C0\",\n",
    "    alpha=0.1,\n",
    "    zorder=0,\n",
    "    label=\"Prediction samples\",\n",
    ")\n",
    "for pred in amfvibnn_preds[:-1]:\n",
    "    axes[1].plot(x_test, pred.detach().numpy(), color=\"C0\", alpha=0.1, zorder=0)\n",
    "axes[1].plot(\n",
    "    x_test,\n",
    "    amfvibnn_preds.detach().mean(0).numpy(),\n",
    "    color=\"C0\",\n",
    "    alpha=1.0,\n",
    "    ls=\"--\",\n",
    "    zorder=0,\n",
    "    label=\"Mean prediction\",\n",
    ")\n",
    "# axes[1].fill_between([x_min, 3], 5, -5, color=\"grey\", alpha=0.2)\n",
    "axes[1].scatter(x, y, color=\"C1\", label=\"Datapoints\", zorder=1, s=10)\n",
    "axes[1].set_title(\"Amortised MFVIBNN\", fontsize=18)\n",
    "axes[1].set_ylim([-5.0, 5.0])\n",
    "axes[1].set_xlim([-5.0, 5.0])\n",
    "axes[1].set_xticklabels([])\n",
    "axes[1].set_xticks([], [])\n",
    "axes[1].set_yticklabels([])\n",
    "axes[1].set_yticks([], [])\n",
    "# axes[1].legend()\n",
    "\n",
    "# CNP\n",
    "pred_mean = cnp_preds.loc.detach().numpy()\n",
    "pred_std = cnp_preds.scale.detach().numpy()\n",
    "axes[2].plot(\n",
    "    x_test,\n",
    "    pred_mean,\n",
    "    color=\"C0\",\n",
    "    alpha=1.0,\n",
    "    ls=\"--\",\n",
    "    zorder=0,\n",
    "    label=\"Mean rediction\",\n",
    ")\n",
    "axes[2].fill_between(\n",
    "    x_test.squeeze(),\n",
    "    (pred_mean + 2 * pred_std).squeeze(),\n",
    "    (pred_mean - 2 * pred_std).squeeze(),\n",
    "    # color=\"C0\",\n",
    "    alpha=0.3,\n",
    "    label=\"95% confidence interval\",\n",
    ")\n",
    "# axes[2].fill_between([x_min, 3], 5, -5, color=\"grey\", alpha=0.2)\n",
    "axes[2].scatter(x, y, color=\"C1\", label=\"Datapoints\", zorder=1, s=10)\n",
    "axes[2].set_title(\"Vanilla CNP\", fontsize=18)\n",
    "axes[2].set_ylim([-5.0, 5.0])\n",
    "axes[2].set_xlim([-5.0, 5.0])\n",
    "axes[2].set_xticklabels([])\n",
    "axes[2].set_xticks([], [])\n",
    "axes[2].set_yticklabels([])\n",
    "axes[2].set_yticks([], [])\n",
    "# axes[2].legend()\n",
    "\n",
    "# Vanilla GIBNN\n",
    "axes[3].plot(\n",
    "    x_test,\n",
    "    v_gibnn_preds[-1].detach().numpy(),\n",
    "    color=\"C0\",\n",
    "    alpha=0.1,\n",
    "    zorder=0,\n",
    "    label=\"Prediction samples\",\n",
    ")\n",
    "for pred in v_gibnn_preds[:-1]:\n",
    "    axes[3].plot(x_test, pred.detach().numpy(), color=\"C0\", alpha=0.1, zorder=0)\n",
    "axes[3].plot(\n",
    "    x_test,\n",
    "    v_gibnn_preds.detach().mean(0).numpy(),\n",
    "    color=\"C0\",\n",
    "    alpha=1.0,\n",
    "    ls=\"--\",\n",
    "    zorder=0,\n",
    "    label=\"Mean prediction\",\n",
    ")\n",
    "\n",
    "axes[3].scatter(x, y, color=\"C1\", label=\"Datapoints\", zorder=1, s=10)\n",
    "axes[3].set_title(\"Vanilla GIBNN\", fontsize=18)\n",
    "axes[3].set_ylim([-5.0, 5.0])\n",
    "axes[3].set_xlim([-5.0, 5.0])\n",
    "axes[3].set_xticklabels([])\n",
    "axes[3].set_xticks([], [])\n",
    "axes[3].set_yticklabels([])\n",
    "axes[3].set_yticks([], [])\n",
    "# axes[3].legend()\n",
    "\n",
    "# NP loss AGIBNN\n",
    "axes[4].plot(\n",
    "    x_test,\n",
    "    npagibnn_preds[-1].detach().numpy(),\n",
    "    color=\"C0\",\n",
    "    alpha=0.1,\n",
    "    zorder=0,\n",
    "    label=\"Prediction samples\",\n",
    ")\n",
    "for pred in npagibnn_preds[:-1]:\n",
    "    axes[4].plot(x_test, pred.detach().numpy(), color=\"C0\", alpha=0.1, zorder=0)\n",
    "axes[4].plot(\n",
    "    x_test,\n",
    "    npagibnn_preds.detach().mean(0).numpy(),\n",
    "    color=\"C0\",\n",
    "    alpha=1.0,\n",
    "    ls=\"--\",\n",
    "    zorder=0,\n",
    "    label=\"Mean prediction\",\n",
    ")\n",
    "# axes[4].fill_between([x_min, 3], 5, -5, color=\"grey\", alpha=0.2)\n",
    "axes[4].scatter(x, y, color=\"C1\", label=\"Datapoints\", zorder=1, s=10)\n",
    "axes[4].set_title(\"Amortised GIBNN, NP Loss\", fontsize=18)\n",
    "axes[4].set_ylim([-5.0, 5.0])\n",
    "axes[4].set_xlim([-5.0, 5.0])\n",
    "axes[4].set_xticklabels([])\n",
    "axes[4].set_xticks([], [])\n",
    "axes[4].set_yticklabels([])\n",
    "axes[4].set_yticks([], [])\n",
    "# axes[4].legend()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('bnn-amort-inf-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "35fb1e0ce207c8143d1ac09f84e572577d5a384bc00c7057b27c48465068398e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
